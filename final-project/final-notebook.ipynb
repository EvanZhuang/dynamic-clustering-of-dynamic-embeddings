{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering on Bernoulli Embeddings with Dynamic Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Yufan Zhuang*<br>\n",
    "*Xuewei Du*<br>\n",
    "*Columbia University*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are powerful in terms of learning latent semantic structure in language.  Recently, Rudolph and Blei (Rudolph and Blei, 2018) explained dynamic embeddings for language evolution, in which he used exponential family embeddings (Rudolph et al., 2018) which can change over time to capture the change in meanings of words over time. Here in this report, we would like to elaborate some findings on modification of the previous models, where we will have context vectors that change over time, and we will have Gaussian Mixture Model(GMM) to cluster words. We study how this new model performs on United Nations general debates data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our contribution is threefold.\n",
    "\n",
    "1. We made the contextual vectors time-dependent, such that it captures the dynamic language evolution in time.\n",
    "2. We built our model in a hybrid setting that stabilizes the training process which could be generalized to other embedding training models.\n",
    "3. We constructed a topic model that outperforms LDA in undirected coherence measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings can be used to represent text data. For instance, in frequency based embedding, if we do one-hot encoding on a particular text data, then we will have a vector of the same length as the number of unique words to represent each word. To represent a particular word, we can set the corresponding position in the vector to 1, and others to 0. However, more generally, we do not necessarily have to use such high dimensional vectors to represent words. We can have one vector of much lower dimension(i.e. a much shorter vector) to represent a word, and each position in that vector can be decimal numbers instead of (0,1). In probabilistic programming setting, each position in this embedding vector can be viewed as a parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word in the observed text is not only dependent upon word embeddings but also its context. This means each word is related to a few words before and after itself. As Rudolph and Blei (Rudolph and Blei, 2018) pointed out in the paper, the typical context has a size between 2 and 10, and each word is modeled conditionally on the words before and after. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rudolph and Blei (Rudolph and Blei, 2018) explained how dynamic embeddings work. In particular, for each time slice, there are some observed words. Each observed word is dependent upon both its embeddings under the same time slice and its context vector which is shared among all time slices. Over time, the embeddings change while the context stays the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are proposing clustering using GMM Bernoulli embeddings and dynamic context vectors. There are practically three components to the model, Bernoulli, dynamic and GMM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dynamic_graph.png\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli means the Bernoulli embedding model (Rudolph and Blei, 2018). Under the Bernoulli embedding model, we have a one-hot encoded embedding vector of length V to represent each word. The corresponding position is 1 for a particular word, and 0 for other positions.  Each position in the embedding vector of each observed word is modeled using Bernoulli distribution with probability $p_{iv}$ for each $v$ in the entire vocabulary $V$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_{iv}|x_{c_i}) = Bern(p_{iv})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log odds $\\eta_{iv} = log \\frac{p_{iv}}{1-p_{iv}}$, which is a function of $p_{iv}$, can be specified as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\eta_{iv} = \\rho_v^T(\\sum_{j \\in c_i}\\sum_{v'}\\alpha_{v'}x_{jv'})$$ Here $\\rho_v \\in \\mathbb{R}^K$ represents embeddings and $\\alpha_{v'} \\in \\mathbb{R}^K$ represents context. The value K is set in advance. Therefore the log-odds is an inner-product between the embeddings and context vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic means dynamic context vector. In particular, the log odds can be specified as the inner-product between the embeddings and **time-specific** context vector. $$\\eta_{iv} = \\rho_v^T(\\sum_{j \\in c_i}\\sum_{v'}\\alpha_{v'}^{t_i}x_{jv'})$$ where $t_i$ is the time slice for the word $x_{iv}$. As we can see in the above graph illustration, we have different alphas for different time slices. <br>\n",
    "The prior on dynamic context vectors is Gaussian random walk, which can be specified as follow: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_v^0 \\sim N(0, \\lambda_0^{-1}I)$$ $$\\alpha_v^t \\sim N(\\alpha_v^{t-1}, \\lambda^{-1}I)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we build a GMM on the embedding vectors $\\rho_v$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\rho_v | \\pi, \\mu, \\Lambda) = \\Pi_{k=1}^{K}(\\pi_k N(\\rho_v | \\mu_k, \\Lambda_k^{-1}))$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to learn parameters $\\pi$, $\\mu$ and $\\Lambda$. Their prior can be set as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\pi) \\sim Dir(\\alpha)$$ $$P(\\mu_k) \\sim N(0, I)$$ $$P(\\Lambda_k) \\sim Wishart(a, B)$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import Normal, Bernoulli\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we use is regarding United Nations general debates (Kaggle.com, 2018). It has transcripts of general debates from 1970 to 2016. Each row in the data represents a speech given by a particular country’s representative in a particular year and session. The raw data has four columns: session, year, country and text. Session means the UN session and there is one session per year. Country means the country which the representative is from. Text means the transcript of the representative’s speech. <br>\n",
    "\n",
    "Preprocessing: Since the data is in years, it is natural to set each time slice to be a year. All the speeches in the same year are attached together. In the resulting data folder, we have one txt file for each year, and each txt file includes all the speeches in that year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'un-general-debates.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3209a642c218>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"un-general-debates.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_speeches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvocab_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'un-general-debates.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Data is not included in the github repo because of its size. \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_df = pd.read_csv(\"un-general-debates.csv\", encoding='utf-8')\n",
    "num_speeches = []\n",
    "vocab_total = set()\n",
    "for single_year in range(1970, 2016):\n",
    "    debate_single_year = data_df.loc[data_df.year == single_year][\"text\"]\n",
    "    # print(len(debate_single_year))\n",
    "    num_speeches.append(len(debate_single_year))\n",
    "    debate_single_year = debate_single_year.str.cat()\n",
    "    debate_single_year = debate_single_year.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    debate_single_year = debate_single_year.replace(\"   \", \" \").replace(\"  \", \" \")\n",
    "    debate_single_year = debate_single_year.replace(\",\", \"\").replace(\".\", \"\")\n",
    "    debate_single_year = debate_single_year.replace(\"?\", \"\").lower()\n",
    "    \n",
    "    vocab_single_year = set(debate_single_year.split())\n",
    "    vocab_total = vocab_total.union(vocab_single_year)\n",
    "    \n",
    "print(\"Total number of vocabulary is about \" + str(len(vocab_total)))\n",
    "print(\"\\n\")\n",
    "print(\"Number of speeches in a year: \\n\" + str(pd.Series(num_speeches).describe()))\n",
    "print(\"Total number of speeches in all years: \" + str(sum(num_speeches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above summary, there are about 70-195 speeches each year. The earlier years had less speeches and the later years had more speeches. There are 7507 speeches in the dataset in total. The number of unique words is roughly 100K. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We offer two inference methods for the clustering, Hamiltonian Monte Carlo (HMC) and Stochastic Gradient Descent Variational Inference (SGDVI) to get the Bayes estimate. For the embedding model, we employ stochastic gradient descent to get the MAP estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hmc_ill.png\"/>\n",
    "(Illustration from Wang et al., 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamiltonian Monte Carlo was proposed in the late 80s as a method to investigate the structure of particles (Duane et al., 1987). It then was borrowed into the statistics community as a very efficient method to approximate the posterior. The basic idea is that instead of moving randomly as we did in MH, we move around a vector field that we have some directions to follow to get to unexplored regions. We utilized the tensorflow probability to implement this as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM_HMC(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observations,\n",
    "            components=3,\n",
    "            run_steps=1000,\n",
    "            leap_steps=3,\n",
    "            burn_in=100):\n",
    "        # Create a mixture of two Gaussians:\n",
    "        import tensorflow as tf\n",
    "        import numpy as np\n",
    "        import tensorflow_probability as tfp\n",
    "        # from tensorflow_probability import edward2 as ed\n",
    "        import functools\n",
    "\n",
    "        tfd = tfp.distributions\n",
    "        tfb = tfp.bijectors\n",
    "        dtype = np.float32\n",
    "        N, dims = observations.shape\n",
    "\n",
    "        def joint_log_prob(observations, mix_probs, loc, chol_precision):\n",
    "            rv_observations = tfd.MixtureSameFamily(\n",
    "                mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
    "                components_distribution=MVNCholPrecisionTriL(\n",
    "                    loc=loc,\n",
    "                    chol_precision_tril=chol_precision))\n",
    "            log_prob_parts = [\n",
    "                rv_observations.log_prob(observations),  # Sum over samples.\n",
    "                pi.log_prob(mix_probs)[..., tf.newaxis],\n",
    "                mu.log_prob(loc),  # Sum over components.\n",
    "                sigma.log_prob(chol_precision),  # Sum over components.\n",
    "            ]\n",
    "            sum_log_prob = tf.reduce_sum(\n",
    "                tf.concat(log_prob_parts, axis=-1), axis=-1)\n",
    "            # Note: for easy debugging, uncomment the following:\n",
    "            # sum_log_prob = tf.Print(sum_log_prob, log_prob_parts)\n",
    "            return sum_log_prob\n",
    "\n",
    "        pi = tfd.Dirichlet(\n",
    "            concentration=np.ones(components, dtype) * 1e3,\n",
    "            name='pi_dist')\n",
    "        mu = tfd.Independent(\n",
    "            tfd.Normal(\n",
    "                loc=np.stack([np.zeros(dims, dtype)] * components),\n",
    "                scale=tf.ones([components, dims], dtype)),\n",
    "            reinterpreted_batch_ndims=1,\n",
    "            name='mu_dist')\n",
    "\n",
    "        sigma = tfd.Wishart(\n",
    "            df=dims,\n",
    "            scale_tril=np.stack([np.eye(dims, dtype=dtype)] * components),\n",
    "            input_output_cholesky=True,\n",
    "            name='sigma_dist')\n",
    "\n",
    "        # testing\n",
    "        unnormalized_posterior_log_prob = functools.partial(\n",
    "            joint_log_prob, observations)\n",
    "        initial_state = [\n",
    "            tf.fill([components],\n",
    "                    value=np.array(1. / components, dtype),\n",
    "                    name='mix_probs'),\n",
    "            tf.constant(np.stack([np.zeros(dims, dtype)] * components),\n",
    "                        name='mu'),\n",
    "            tf.eye(dims, batch_shape=[components],\n",
    "                   dtype=dtype, name='chol_precision'),\n",
    "        ]\n",
    "        unconstraining_bijectors = [\n",
    "            tfb.SoftmaxCentered(),\n",
    "            tfb.Identity(),\n",
    "            tfb.Chain([\n",
    "                tfb.TransformDiagonal(tfb.Softplus()),\n",
    "                tfb.FillTriangular(),\n",
    "            ])]\n",
    "        with tf.variable_scope(\"alice_world\", reuse=tf.AUTO_REUSE):\n",
    "            mcstep_size = tf.get_variable(\n",
    "                name='step_size',\n",
    "                initializer=np.array(0.05, dtype),\n",
    "                use_resource=True,\n",
    "                trainable=False)\n",
    "            [mix_probs, loc, chol_precision],\\\n",
    "                kernel_results = tfp.mcmc.sample_chain(\n",
    "                    num_results=run_steps,\n",
    "                    num_burnin_steps=burn_in,\n",
    "                    current_state=initial_state,\n",
    "                    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
    "                        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "                            target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "                            step_size=mcstep_size,\n",
    "                            step_size_update_fn=tfp.mcmc.\n",
    "                            make_simple_step_size_update_policy(),\n",
    "                            num_leapfrog_steps=leap_steps),\n",
    "                        bijector=unconstraining_bijectors))\n",
    "\n",
    "            acceptance_rate = tf.reduce_mean(tf.to_float(\n",
    "                kernel_results.inner_results.is_accepted))\n",
    "            mean_mix_probs = tf.reduce_mean(mix_probs, axis=0)\n",
    "            mean_loc = tf.reduce_mean(loc, axis=0)\n",
    "            mean_chol_precision = tf.reduce_mean(chol_precision, axis=0)\n",
    "\n",
    "            obs = tf.convert_to_tensor(observations)\n",
    "            c = tf.get_variable(\"c\",\n",
    "                                [len(observations)],\n",
    "                                dtype=dtype,\n",
    "                                initializer=tf.zeros_initializer)\n",
    "            c_log = tf.get_variable(\"c_log\",\n",
    "                                    [len(observations)],\n",
    "                                    dtype=dtype,\n",
    "                                    initializer=tf.zeros_initializer)\n",
    "            # tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            def condition(i, px): return i < len(observations)\n",
    "\n",
    "            def c_assign(i, c):\n",
    "                theta = tfd.MultivariateNormalTriL(\n",
    "                    loc=mean_loc, scale_tril=mean_chol_precision)\n",
    "                c_log = tf.get_variable(\"c_log\")\n",
    "                c_log = tf.scatter_update(\n",
    "                    c_log,\n",
    "                    indices=i,\n",
    "                    updates=tf.reduce_max(\n",
    "                        tf.log(mean_mix_probs) +\n",
    "                        theta.log_prob(\n",
    "                            obs[i])))\n",
    "                c = tf.get_variable(\"c\")\n",
    "                c = tf.scatter_update(\n",
    "                    c,\n",
    "                    indices=i,\n",
    "                    updates=tf.to_float(\n",
    "                        tf.argmax(\n",
    "                            tf.log(mean_mix_probs) +\n",
    "                            theta.log_prob(\n",
    "                                obs[i]))))\n",
    "                return i + 1, c\n",
    "\n",
    "            i, c = tf.while_loop(condition, c_assign, [0, c])\n",
    "        with tf.Session() as sess:\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            # sess.run(init_op)\n",
    "            [\n",
    "                self.acceptance_rate_,\n",
    "                self.pi_out,\n",
    "                self.mu_out,\n",
    "                self.sigma_out,\n",
    "                self.assignment,\n",
    "                self.assignment_likelihood\n",
    "            ] = sess.run([\n",
    "                acceptance_rate,\n",
    "                mean_mix_probs,\n",
    "                mean_loc,\n",
    "                mean_chol_precision,\n",
    "                c,\n",
    "                c_log\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our setting, we can make the conjugate exponential family (CEF) argument such that under the mean-field assumption, the variational distributions will be in the same form as the distributions that they are trying to approximate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "p(\\pi,\\mu,\\Lambda, c|\\rho) \\approx q(\\pi)[\\prod_{j=1}^{K} q(\\mu_j)q(\\Lambda_j)][\\prod_{i=1}^{n} q(c_i)]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "q(\\pi) &= Dir(\\alpha_1', \\alpha_2',...,\\alpha_k') \\\\\n",
    "q(\\mu_j) &= N(m_j',\\Sigma_j') \\\\\n",
    "q(\\Lambda_j) &= Wishart(a_j',B_j')  \\\\\n",
    "q(c_i) &= Discrete(\\phi_i) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELBO is then\n",
    "$$\n",
    "L = E_q[\\ln p(\\rho,\\pi,\\mu,\\Lambda, c) - \\ln q ] + const.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical optimal update schemes can be derived, but since we are dealing with high-dimensional big data, we choose to use SGD methods instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods are both implemented as classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticizm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we built an embedding model and a topic model at the same time, we will evaluate it in two metrics, the log-likelihood and the topic coherence. \n",
    "\n",
    "For the log-likelihood, we compare the likelihood for the positive samples among our model, the dynamic Bernoulli embedding model, and the Bernoulli embedding model.\n",
    "\n",
    "Regarding the topic coherence, it is difficult to define it explicitly so there are multiple indicators come from different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train on top **10000 words, with 5000 training steps, 10 mixture components and embedding dimension of 50**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process and log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our Model (with HMC)\n",
    "<img src=\"DCEMB_LOSS.png\" style=\"display:inline;margin:1px;width:49%;\" />\n",
    "<img src=\"DCEMB_MIX.png\"  style=\"display:inline;margin:1px;width:49%;\"/>\n",
    "#### Dynamic Embedding Model and Bournoulli Embedding Model\n",
    "<img src=\"DEMB_LOSS.png\"  style=\"display:inline;margin:1px;width:49%;\"/>\n",
    "<img src=\"EMB_LOSS.png\"   style=\"display:inline;margin:1px;width:49%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $C_{uci}$\n",
    "\n",
    "It measures the co-occurrence probability of every word pairs in a topic (Newman et al., 2010).\n",
    "Suppose we have a topic of three words {one, two, three}. The co-occurrence probability of any two words would be calculated based on sliding windows, for example, if our text is \"One is Two\", the virtual documents with a size 2 sliding window would be {\"One is\", \"is Two\"}.\n",
    "\n",
    "In this case, P(one) = $\\frac{1}{2}$  (appeared once in two virtual documents), P(one, two) = 0 (no co-occurrence of one and two)\n",
    "$$PMI(one, two) = \\frac{log(P(one,two)+\\epsilon)}{P(one)\\cdot P(two)} $$($\\epsilon$ is for smoothing, otherwise we get log(0))\n",
    "\n",
    "$$C_{uci} = \\frac{1}{3}\\cdot[PMI(one, two) + PMI(one, three) + PMI(two, three)]$$\n",
    "\n",
    "#### $U_{mass}$\n",
    "\n",
    "It measures the conditional probability of weaker words given stronger words in a topic (Mimno et al., 2011). The idea is that the occurrence of every top word should be supported by every preceding top word.\n",
    "Followed by our previous {one, two, three} topic.\n",
    "\n",
    "$$U_{mass}  = \\frac{1}{3}\\cdot( K(two|one) + K(three|one) + K(three|two))$$ where $$K(two|one) = \\frac{log (P(two, one) + 1)}{P(one)}$$\n",
    "\n",
    "#### $C_{npmi}$\n",
    "\n",
    "It is an improved version of $C_{uci}$ by adding normalization (Aletras & Stevenson, 2013).\n",
    "It is just like $C_{uci}$ but changed PMI to NPMI (normalized PMI), which is listed below:\n",
    "$$NPMI(one, two)  = (\\frac{\\frac{log( P(one,two)+1)}{P(one)\\cdot P(two)}}{-log(P(one,two)+1)})^\\gamma $$\n",
    "(An increase of $\\gamma$ gives higher NPMI values more weight, $\\gamma$ = 1 is a common choice)\n",
    "\n",
    "#### $C_v$\n",
    "\n",
    "It measures the indirect similarity among words in a topic (Röder et al., 2015).\n",
    "Some words belong to the same topic but they rarely occur together. But the words around them should be similar. For example, suppose we have \"McDonald makes chicken nuggets\" and \"KFC serves chicken nuggets\", we will probably want to put McDonald and KFC together. That's the intuition behind indirect similarity.\n",
    "\n",
    "However, the math is a bit complicated here.\n",
    "\n",
    "The co-occurrence counts are used to calculated the NPMI of every top word to every other top word, thus, resulting in a set of vectors, one for every top word. The calculation of the similarity is done between every top word vector and the sum of all top word vectors. As similarity measure, the cosinus is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Coherence Measure | DCEMB   |   LDA   |\n",
    "|------|------|------|\n",
    "|   $C_{uci}$  | -7.8974 |**-0.1428**|\n",
    "|   $U_{mass}$  | -6.7553 |**-0.0001**|\n",
    "|   $C_{npmi}$  | -0.2838 |**-0.0388**|\n",
    "|   $C_v$  | **0.4300** |0.2892|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the resulted topics as well as the contextual change of words over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Visualization aftering Mapping to $\\mathbb{R}^2$ with TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rho_1.png\" style=\"width:110%;\">\n",
    "<img src=\"rho.png\" style=\"display:inline;margin:1px;width:110%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 words for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Topic 1 | Topic 2  |   Topic 3  |Topic 4 | Topic 5   |   Topic 6   |Topic 7 | Topic 8   |   Topic 9   |Topic 10|\n",
    "|------|------|------|------|------|------|------|------|------|------|\n",
    "|   fruition | lome | unfulfilled |  andean | subjugation |leone|   tensions  | korean |strongly|   the  |\n",
    "|  migratory  | forceful |uncontrolled| candidature |culminated |denied| achievements  | concept |critical|   of |\n",
    "|  minute  | stature |unjustified| gloomy | obtaining |training|  attempts  | protect|obligations|   and  |\n",
    "|   constituting | commits |spiral|  optional| electricity |white|  addition  | moral |reiterate|  to  |\n",
    "|   daughters  | sadat |blacks| unanimity  |amaral |quickly|   somalia  | court |causes|  in  |\n",
    "|  prudence | dividends |dictate| thant  | differing |bolivia|  borders | millions |participate|   a  |\n",
    "|  champions | thorn |pernicious|  expansionism | agendas |sister|  constitute | around |representatives|   that |\n",
    "|  com  | interpretations |deplores|  persisted  | wasted |inhabitants|  signed | grave |above|   is  |\n",
    "|  edge | rescheduling |graduation|  nationally  |flourish |destroy|  weapon  | light |presence|  for  |\n",
    "|  impatience | nassir |aviv|  steer  | understandable |eradicate|  stage | discrimination |outside|   we |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wang, Z., M. Broccardo, and J. Song* (2019). Hamiltonian Monte Carlo methods for subset simulation in reliability analysis. Structural Safety. Vol. 76, 51-67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rudolph, M., & Blei, D. (2018, April). Dynamic Embeddings for Language Evolution. In Proceedings of the 2018 World Wide Web Conference on World Wide Web (pp. 1003-1011). International World Wide Web Conferences Steering Committee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rudolph, M., Ruiz, F., Mandt, S., & Blei, D. (2016). Exponential family embeddings. In Advances in Neural Information Processing Systems (pp. 478-486)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle.com. (2018). UN General Debates. [online] Available at: https://www.kaggle.com/unitednations/un-general-debates [Accessed 27 Nov. 2018]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987). Hybrid monte carlo. Physics letters B, 195(2), 216-222."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newman, D., Lau, J. H., Grieser, K., & Baldwin, T. (2010, June). Automatic evaluation of topic coherence. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 100-108). Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mimno, D., Wallach, H. M., Talley, E., Leenders, M., & McCallum, A. (2011, July). Optimizing semantic coherence in topic models. In Proceedings of the conference on empirical methods in natural language processing (pp. 262-272). Association for Computational Linguistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aletras, N., & Stevenson, M. (2013). Evaluating topic coherence using distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013)–Long Papers (pp. 13-22)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Röder, M., Both, A., & Hinneburg, A. (2015, February). Exploring the space of topic coherence measures. In Proceedings of the eighth ACM international conference on Web search and data mining (pp. 399-408). ACM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
